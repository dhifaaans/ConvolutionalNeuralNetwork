# -*- coding: utf-8 -*-
"""cnn.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fddPde8q4mAsfJMvxlAt_nNOuusq9_7t
"""

#"" Convolutional Neural Network.
#Build and train a convolutional neural network with TensorFlow.
#This example is using the MNIST database of handwritten digits
#(http://yann.lecun.com/exdb/mnist/)
#This example is using TensorFlow layers API, see 'convolutional_network_raw' 
#example for a raw implementation with variables.
#Author: Aymeric Damien
#Project: https://github.com/aymericdamien/TensorFlow-Examples/
#"""

from __future__ import division, print_function, absolute_import

# Import MNIST data
from tensorflow.examples.tutorials.mnist import input_data
mnist = input_data.read_data_sets("/tmp/data/", one_hot=False)

import tensorflow as tf
import matplotlib.pyplot as plt

# Training Parameters
learning_rate = 0.001
epoch = 100
batch_size = 128

# Network Parameters
num_input = 784 # MNIST data input (img shape: 28*28)
num_classes = 10 # MNIST total classes (0-9 digits)
dropout = 0.25 # Dropout, probability to drop a unit


# Create the neural network
def conv_net(x_dict, n_classes, dropout, reuse, is_training):
    # Define a scope for reusing the variables
    with tf.variable_scope('ConvNet', reuse=reuse):
        # TF Estimator input is a dict, in case of multiple inputs
        x = x_dict['images']

        # MNIST data input is a 1-D vector of 784 features (28*28 pixels)
        # Reshape to match picture format [Height x Width x Channel]
        # Tensor input become 4-D: [Batch Size, Height, Width, Channel]
        x = tf.reshape(x, shape=[-1, 28, 28, 1])

        # Convolution Layer with 32 filters and a kernel size of 5
        conv1 = tf.layers.conv2d(x, 32, 5, activation=tf.nn.relu)
        # Max Pooling (down-sampling) with strides of 2 and kernel size of 2
        conv1 = tf.layers.max_pooling2d(conv1, 2, 2)

        # Convolution Layer with 64 filters and a kernel size of 3
        conv2 = tf.layers.conv2d(conv1, 64, 3, activation=tf.nn.relu)
        # Max Pooling (down-sampling) with strides of 2 and kernel size of 2
        conv2 = tf.layers.max_pooling2d(conv2, 2, 2)

        # Flatten the data to a 1-D vector for the fully connected layer
        fc1 = tf.contrib.layers.flatten(conv2)

        # Fully connected layer (in tf contrib folder for now)
        fc1 = tf.layers.dense(fc1, 1024)
        # Apply Dropout (if is_training is False, dropout is not applied)
        fc1 = tf.layers.dropout(fc1, rate=dropout, training=is_training)

        # Output layer, class prediction
        out = tf.layers.dense(fc1, n_classes)

    return out


# Define the model function (following TF Estimator Template)
def model_fn(features, labels, mode):
    # Build the neural network
    # Because Dropout have different behavior at training and prediction time, we
    # need to create 2 distinct computation graphs that still share the same weights.
    logits_train = conv_net(features, num_classes, dropout, reuse=False,
                            is_training=True)
    logits_test = conv_net(features, num_classes, dropout, reuse=True,
                           is_training=False)

    # Predictions
    pred_classes = tf.argmax(logits_test, axis=1)
    pred_probas = tf.nn.softmax(logits_test)

    # If prediction mode, early return
    if mode == tf.estimator.ModeKeys.PREDICT:
        return tf.estimator.EstimatorSpec(mode, predictions=pred_classes)

        # Define loss and optimizer
    loss_op = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(
        logits=logits_train, labels=tf.cast(labels, dtype=tf.int32)))
    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)
    train_op = optimizer.minimize(loss_op,
                                  global_step=tf.train.get_global_step())

    # Evaluate the accuracy of the model
    acc_op = tf.metrics.accuracy(labels=labels, predictions=pred_classes)

    # TF Estimators requires to return a EstimatorSpec, that specify
    # the different ops for training, evaluating, ...
    estim_specs = tf.estimator.EstimatorSpec(
        mode=mode,
        predictions=pred_classes,
        loss=loss_op,
        train_op=train_op,
        eval_metric_ops={'accuracy': acc_op})

    return estim_specs

# Build the Estimator
model = tf.estimator.Estimator(model_fn)

# Define the input function for training
trainInput_fn = tf.estimator.inputs.numpy_input_fn(
    x={'images': mnist.train.images}, y=mnist.train.labels,
    batch_size=batch_size, num_epochs=None, shuffle=True)

# Define the input function for evaluating
testInput_fn = tf.estimator.inputs.numpy_input_fn(
    x={'images': mnist.test.images}, y=mnist.test.labels,
    batch_size=batch_size, shuffle=False)

##===========================================================================##
for i in range(1, (epoch+1)):
  # Train the Model
  model.train(trainInput_fn, steps=1)
  
  # Evaluate the Model
  # Use the Estimator 'evaluate' method
  e = model.evaluate(testInput_fn)
  print("Akurasi: {}%\n".format(e['accuracy'] * 100))

errorTrain = [2.2956505,2.2586243,1.9832978,1.8384674,1.6684334,1.5032898,1.1904058,1.0014042,0.9099817,0.7546259,0.52720225,0.56466705,0.35757422,0.5872927,0.4392279,0.4416437,0.5452827,0.36083162,0.4790209,0.46454048,0.38625735,0.46072632,0.39381528,0.28401145,0.39149088,0.50128645,0.388711,0.4022245,0.2949235,0.23138902,0.4063921,0.20464069,0.2425396,0.42047662,0.120589815,0.1935586,0.19603795,0.19860587,0.2350606,0.29514122,0.19629928,0.19000824,0.16731554,0.1773208,0.1624068,0.18685308,0.15194824,0.120993644,0.06350738,0.19544576,0.30352294,0.12394539,0.12285087,0.21857786,0.11989367,0.062642895,0.14169946,0.16682884,0.19145852,0.15630609,0.13803653,0.17985867,0.12316211,0.27331674,0.0557655,0.12312862,0.03650687,0.2540843,0.13407168,0.26007882,0.14738852,0.123038836,0.052249424,0.06558124,0.20086591,0.14756209,0.25920492,0.2250861,0.1394643,0.14647302,0.117566004,0.11544737,0.089264676,0.10889454,0.22551374,0.038638957,0.0846834,0.089613736,0.061838463,0.08527001,0.1955469,0.19981518,0.096333615,0.22286637,0.059638284,0.12613484,0.14817399,0.10402237,0.41928643,0.2253562]

errorValidate = [
2.239073,2.0333133,1.8729126,1.6855751,1.4336164,1.1682407,1.023354,0.88939995,0.72155213,0.58121336,0.5551462,0.5596241,0.5370408,0.45490998,0.40863582,0.39094162,0.4119054,0.3947794,0.36491847,0.33069292,0.32821587,0.32693464,0.3424422,0.3555203,0.33632195,0.29347247,0.27083328,0.25904343,0.25692093,0.26063672,0.24827878,0.23754166,0.22983254,0.21926215,0.21103154,0.20452438,0.19777752,0.19140148,0.18418805,0.18497002,0.18465795,0.18398987,0.18099903,0.17866424,0.17749645,0.17137772,0.16764504,0.16664611,0.16576691,0.1635203,0.16020948,0.16336194,0.17030394,0.16139635,0.1582018,0.15419963,0.1486987,0.14721952,0.14878874,0.15410231,0.15946136,0.15527566,0.13887165,0.12727487,0.12988932,0.13860089,0.14923187,0.14957418,0.14602669,0.13013451,0.12296616,0.12497118,0.1333327,0.14205241,0.13134216,0.1235516,0.114087395,0.11218492,0.11697041,0.12535615,0.121979654,0.119464286,0.11184541,0.105390124,0.102779426,0.10339955,0.11046166,0.11550888,0.12273037,0.13001932,0.12621532,0.11755347,0.10885881,0.1013165,0.09752966,0.09538804,0.09987803,0.1093619,0.116873935,0.11702179]



plt.plot(errorTrain, label = "training")
plt.plot(errorValidate, label = "testing")
plt.legend()
plt.ylabel('Error')
plt.xlabel('Epoch')
plt.title('CNN Error Graph')
plt.show()

theAccuracy = [0.1747,0.5043,0.4408,0.5128,0.6863,0.8489,0.811 ,0.7875,0.8251,0.8611,0.8464,0.8302,0.8324,0.8673,0.8818,0.8842,0.8778,0.8836,0.8909,0.9045,0.9085,0.9041,0.8984,0.8949,0.906 ,0.9205,0.9278,0.9289,0.9252,0.9225,0.9269,0.9326,0.9354,0.9358,0.9372,0.9404,0.9427,0.9465,0.949 ,0.9504,0.9519,0.9519,0.9524,0.952 ,0.9524,0.9538,0.9545,0.9547,0.9551,0.9548,0.9547,0.9545,0.954 ,0.955 ,0.9572,0.9586,0.9582,0.9593,0.9588,0.9558,0.9539,0.9551,0.96  ,0.9632,0.9641,0.9617,0.9587,0.9578,0.9577,0.9619,0.9636,0.9605,0.9585,0.9551,0.9592,0.9632,0.9666,0.9685,0.969 ,0.9649,0.9661,0.9678,0.9701,0.9709,0.9706,0.969 ,0.9676,0.9659,0.962 ,0.9581,0.96  ,0.9647,0.9671,0.9701,0.9724,0.9736,0.9715,0.9692,0.9679,0.9677]

import matplotlib.pyplot as plt
plt.plot(theAccuracy, label = "Akurasi")
plt.ylabel('Akurasi (%)')
plt.xlabel('Epoch')
plt.title('Grafik Akurasi')
plt.show()
print("Akurasi akhir (dalam 100%): {}".format(theAccuracy[99]*100))